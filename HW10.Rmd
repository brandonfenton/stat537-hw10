---
title: "STAT 537 HW 10"
author: "Doug Anderson and Brandon Fenton"
date: "Due April 28, 2016"
output:
  pdf_document: default
  fig_width: 8
  fig_height: 5
  word_document: null
---
```{r setup, echo=F, warning=F, message=F}
require(rpart)
require(party)
require(partykit)
require(openintro)
require(pander)

# In Marshall et al. (http://www.sciencedirect.com/science/article/pii/S1063458405001524), they use logistic regression models to build and then validate (to some degree) a predictor for osteoarthritis/not using a set of genetic markers. Read the paper to answer the following questions.
# 
# Read the paper, focusing on the statistical aspects of their work.
```

Part 1: Classification Trees for the diagnosis of Mild Osteoarthritis using genetic information. 

1) Fit their top ranked model from Supplemental Table 5. Make effects plots for the resulting estimated model using the following code.

```{r p1_a, echo=F, fig.height=10, fig.width=10, warning=F, message=F} 
set.seed(888)
marsh1 <- read.csv("marshalcomb.csv",header=T)
# View(marsh1)
# summary(marsh1)
marsh1R<-marsh1[,-c(1,2,3,4,5,6,8)]
# summary(marsh1R)

#Fit the model and call it glm1
## I thought this was right but numbers don't match
glm1 <- glm(D2~G2AN+IKBKAP+IL13RA1+LAMC1+MAFB+PF4+TNFAIP6, 
            data=marsh1R, family=binomial(link = "logit"))

require(effects)
plot(allEffects(glm1),type="link") 
```

2) They use logit values above or below 0 to predict whether an observation is diseased or not (below equation 1). Explain that choice.

$\text{logit}(p) = \log(\frac{p}{1-p})$, and $\log(\frac{0.5}{1-0.5}) =\log(1)=0$, so they are using a probability of 0.5 as the decision boundary for concluding the predicted diagnosis.  This is the typical value used as a cutoff in logistic regression.

3) In the section on page 865 “Reference data set (AD1F2) for the best gene combinations” describe the type of modeling/model selection they are considering. Supplement table 5 contains more details of the results of this process.

All of the 511 possible biomarker combinations were fit as models and compared using ROC curves.  68 models which were "well-behaved" TODO in logistic regression and had ROC AUC values higher than 0.90 were retained for use on the test set.

4) Describe what they are doing in the section “Prospective (Blind) test” starting on page 864. Use terminology from JWHT.

This section describes their test set, which was used to assess the performance of the model fit on the training set, AD1F2.  Separate test sets are used so as to gauge the generalizability of a model's predictive ability, since an overfit model would perform well on its test set but not other data.

5) Fit a classification tree using rpart using all 9 of the genetic variables listed below using minbucket=4 and cp=0.000000001 as in the code below. Prune the tree using the Min CV rule after consulting multiple calls to printcp() and plotcp(). Do not load mvpart before doing this problem. Discuss the results of your CV process and how you chose your tree size. 


```{r p5_a, message=F, echo=F, warning=F}
require(rpart)

tree1<-rpart(factor(D2)~.,data=marsh1R,cp=0.000000001,minbucket=4)
plotcp(tree1)

```

```{r p5_b, message=F, echo=F, warning=F, include=F}
# You can also see the Min CV choice in repeated calls using:
ret <- rep(NA, 100)
for (i in 1:100) {
  tree1 <- rpart(factor(D2) ~ ., data = marsh1R, cp = 1e-09, minbucket = 4)
  a <- printcp(tree1)
  ret[i] <- a[which(a[, 4] == min(a[, 4])), 2] + 1
}
```

```{r p5_c, message=F, echo=F, warning=F}
require(pander)

ret.tab <- table(ret)
# emphasize.strong.cells(which.max(ret.tab, arr.ind=T))
emphasize.strong.cols(1)
# TODO fix this
pander(ret.tab, caption="Tree sizes that produced min-CV error out of 100 recursive partitions")
```

Based on 100 recursive partitions (results in Table 1), it seems that trees of size 2 (one split) and size 6 (5 splits) would be appropriate for this data in obtaining the min-CV.


6) Based on these results, choose two different sized trees that could be reasonable based on the different CV selections. Report a plot of your two pruned classification trees using the partykit’s plot(as.party(PRUNEDTREENAME)). Discuss the differences in the two trees.

```{r p6_a, echo=F, fig.height=10, fig.width=10}
# One split
tree1p<-rpart(factor(D2)~.,data=marsh1R,cp=0.19, minbucket=4)

# Five splits
tree2p<-rpart(factor(D2)~.,data=marsh1R,cp=0.039, minbucket=4)

plot(as.party(tree1p))
plot(as.party(tree2p))
```
The size 6 (or 5 splits) tree has four more splits than the size 2 tree.  Both start with a split on PF4 at 6.373, but the second tree contains four splits on LAMC1 (11.665), IKBKAP (9.389), MAFB (7.794) and EGR1 (8.485) in that order on the branch the tree corresponding to PF4 values less than 6.373. While both trees have produced impure classifications, purity is generally higher in the 5-split tree.

7) Adjust the following code as needed to make a plot of the predictions from the three models. Use these results to discuss the differences in these approaches/results. I had models called tree1p, tree2p, and glm1.

```{r p7_a, echo=F, fig.height=10, fig.width=10}
 fits<-data.frame(ONESPLIT=predict(tree1p)[,2], FIVESPLITS=predict(tree2p)[,2],GLM1=predict(glm1,type="response"))

 

 
 fits.tab <- apply(fits,2,function(x) mean(x>0.5))
 names(fits.tab) <- c("One Split", "Five Splits", "GLM Fit")
 pander(fits.tab, caption="Proportion of predicted diagnoses for two classification trees and a GLM model")
```

```{r p7_b, echo=F}
  plot(fits)
```

The glm is predicting a possibly unique probability for each observation, while the trees predict one of either two or six discrete probabilities depending on the tree size, as evidenced by the scatterplot matrix.  However, the proportions of diagnoses predicted by the five-split tree and the glm fit are closer than either proportion is to the proportion predicted by the one-split tree.  

8) The predictor variables were log10-transformed. Generally discuss how this impacts the tree-based approach vs the GLM approach. You can either undo their transformations and refit models or just discuss the impacts based on thinking about how the models work and why they might have done this transformation initially.

Log transformations are used when regressing on right-skewed data, in order to satisfy the assumptions necessary to apply a generalized linear model.  Tree-based methods do not rely on the same assumptions and are less influenced by a lack of normality, since split points can easily be placed at any arbitrary locations.  If we refit the same three models compared in problem 6 with back-transformed predictors, the trees produce equivalent results (the split points are the same as in the first trees if log10-transformed) but the GLM's proportion of predicted diagnoses has dropped by almost 0.05.

```{r p8_a, fig.height=10, fig.width=10, echo=F}
exp.dat <- data.frame(cbind(D2 = marsh1R$D2, apply(marsh1R[,2:10], 2, function(x) 10^x)))

tree3p<-rpart(factor(D2)~.,data=exp.dat,cp=0.19, minbucket=4)
plot(as.party(tree3p))

tree4p<-rpart(factor(D2)~.,data=exp.dat,cp=0.039, minbucket=4)
plot(as.party(tree4p))

glm2 <- glm(D2~G2AN+IKBKAP+IL13RA1+LAMC1+MAFB+PF4+TNFAIP6, 
            data=exp.dat, family=binomial(link = "logit"))
```

```{r p8_b, fig.height=10, fig.width=10}
fits.exp<-data.frame(ONESPLIT=predict(tree3p)[,2], FIVESPLITS=predict(tree4p)[,2],GLM2=predict(glm2,type="response"))
exp.tab <- apply(fits.exp,2,function(x) mean(x>0.5))
names(exp.tab) <- c("One Split", "Five Splits", "GLM Fit")
pander(exp.tab, caption="Proportion of predicted diagnoses models using untransformed predictors") 

```

Part 2: Use the following code to identify an optimal predictive model for first year college GPAs. The following code will split out half the observations into a training data set and fit two different conditional inference trees using all available predictors. It also fits a recursive partitioning tree. 

9) Prune the tree using the 1SE and Min CV rules (either from a single 10-fold CV run or select a tree size for each as a consensus that you build from multiple CV runs). 

```{r p9_a, include=F}
data(satGPA)
set.seed(123456) #So that you can repeat the running of the code and get the same results
train=sample(1:1000,size=500)
ctree1<-ctree(FYGPA~.,data=satGPA[train,],mincriterion=0.95)
ctree2<-ctree(FYGPA~.,data=satGPA[train,],mincriterion=0.9)

# rpart_full<-rpart(FYGPA~.,data=satGPA[train,],cp=0.00000001)

gpa.tree <- rpart(FYGPA ~ ., data = satGPA[train,], cp = 1e-09)

mcv.sizes <- rep(NA, 100)
ose.sizes <- rep(NA, 100)

for (i in 1:100) {
  gpa.tree <- rpart(FYGPA ~ ., data = satGPA[train,], cp = 1e-09)
  a <- printcp(gpa.tree)
  mincv <- a[which(a[, 4] == min(a[, 4])), 4]
  mincv.se <- a[which(a[, 4] == mincv), 5]
  mcv.sizes[i] <- a[which(a[, 4] == mincv), 2] + 1
  ose.sizes[i] <- a[which(a[, 4] < mincv + mincv.se), 2][1] + 1
}
```

```{r p9_b, echo=F, fig.height=10, fig.width=20}
plotcp(gpa.tree)
```

A call to __plotcp()__ on one tree fit suggests that a tree size of 4 should be used by the 1-SE rule, and a tree size of 6 should be used by the min-CV rule.  Fitting 100 cross-validated recursive partitions provides support for sizes of 4 and somewhere between 7 and 9 by the two rules:

```{r p9_c, echo=F}
mcv.tab <- table(mcv.sizes)
ose.tab <- table(ose.sizes)

pander(mcv.tab, caption="Tree sizes that produced min-CV error out of 100 recursive partitions")
pander(ose.tab, caption="Tree sizes determined by the 1-SE rule out of 100 recursive partitions")

gpa.ose <-rpart(FYGPA~.,data=satGPA,cp=3e-02)
gpa.mcv <-rpart(FYGPA~.,data=satGPA,cp=1.002519e-02)
```

The original tree size chosen by the min-CV rule was 9, but producing a size 9 tree proved very difficult and so a size of 8 was used instead.  This seems reasonable given the large numbers of min-CV values associated with sizes 7 and 9 in the results from the 100 recursive partitions generated.

```{r p9_d, echo=F, fig.height=10, fig.width=10}
plot(as.party(gpa.ose))
plot(as.party(gpa.mcv))
```

10) Calculate and compare the validation error for the 4 models using the withheld responses and discuss the choice of significance threshold and CV rule.

```{r p10_a, echo=F}

#Do your own pruning and put results into rpart21SE and rpart2MinCV
pred.t1 <- predict(ctree1,newdata=satGPA[-train,])
pred.t2 <- predict(ctree2,newdata=satGPA[-train,])
pred.ose <- predict(gpa.ose,newdata=satGPA[-train,])
pred.mcv <- predict(gpa.mcv,newdata=satGPA[-train,])

mse.t1 <- mean((satGPA[-train,]$FYGPA - pred.t1)^2)
mse.t2 <- mean((satGPA[-train,]$FYGPA - pred.t2)^2)
mse.ose <- mean((satGPA[-train,]$FYGPA - pred.ose)^2)
mse.mcv <- mean((satGPA[-train,]$FYGPA - pred.mcv)^2)

mse.tab <- cbind(mse.t1, mse.t2, mse.ose, mse.mcv)

colnames(mse.tab) <- c("mincriterion=0.95", "mincriterion=0.90", "1-SE", "Min-CV")
emphasize.strong.cells(which(mse.tab==min(mse.tab), arr.ind=TRUE))
pander(mse.tab)
```

The min-CV tree produced the smallest test error with an MSE of 0.3506.  The largest MSE value, associated with the mincriterion=0.95 fit, was 0.3898.  This does not represent a large difference, so it would appear that all four methods produce similarly-performing results for these data.

## R Code Appendix:
Problem 1:
```{r a1, ref.label='p1_a', eval=F, echo=T}
```

Problem 5:
```{r a5, ref.label='p5_a', eval=F, echo=T}
```

```{r a5b, ref.label='p5_b', eval=F, echo=T}
```

```{r a5c, ref.label='p5_c', eval=F, echo=T}
```

Problem 6:
```{r a6, ref.label='p6_a', eval=F, echo=T}
```

Problem 8:
```{r a8, ref.label='p8_a', eval=F, echo=T}
```

Problem 9:
```{r a9, ref.label='p9_a', eval=F, echo=T}
```

Problem 10:
```{r a10, ref.label='p10_a', eval=F, echo=T}
```
